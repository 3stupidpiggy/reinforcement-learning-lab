# Introduction

Just implement the Actor-Critic(AC), the Advantage Actor-Critic(A2C) and Asynchronous Advantage Actor-Critic(A3C) in the simple environment CatPole-v0. The implemented code is not generic and now just test in this simple environment CartPole-v0

# How to train

Just run the program ac.py, a2c.py or a3c.py

# How to test

Just run the test.py

# The results demonstrate

The fellows are only the reward curve and you can find more in the "Actor-Critic-Algorithm-Learning\CartPole-v0" directory.

![reward_curve(AC)](https://github.com/sunwuzhou03/rl_learning/blob/master/Actor-Critic-Algorithm-Learning/CartPole-v0/reward_curve(AC).png) 

![reward_curve(A2C)](https://github.com/sunwuzhou03/rl_learning/blob/master/Actor-Critic-Algorithm-Learning/CartPole-v0/reward_curve(A2C).png) 

![reward_curve(A3C)](https://github.com/sunwuzhou03/rl_learning/blob/master/Actor-Critic-Algorithm-Learning/CartPole-v0/reward_curve(A3C).png)